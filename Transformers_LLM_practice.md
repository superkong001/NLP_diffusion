![84b043c179aaad94406f9182af81c47b_seq2seq](https://github.com/superkong001/NLP_diffusion/assets/37318654/9af25a38-f7d1-41da-a5c5-49a744122eb0)参考：Transformers技术解析+实战(LLM)，多种Transformers diffusion模型技术图像生成技术+实战
> https://github.com/datawhalechina/sora-tutorial/blob/main/docs/chapter2/chapter2_2.md

# SelfAttention 

![84b043c179aaad94406f9182af81c47b_seq2seq](https://github.com/superkong001/NLP_diffusion/assets/37318654/1f1f31cf-c9e0-43e9-9361-20842ac6576e)

加性Attention，如（Bahdanau attention）：

乘性Attention，如（Luong attention）：

\boldsymbol{v}_a^{\top} \tanh \left(\boldsymbol{W}_{\mathbf{1}} \boldsymbol{h}_t+\boldsymbol{W}_{\mathbf{2}} \overline{\boldsymbol{h}}_s\right)


# LLM
